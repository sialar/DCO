{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016)</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Hands-on Exercises - Outline</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Introduce the basics of convex and distributed optimization**\n",
    "- **Introduce the basics of Apache Spark**\n",
    "\n",
    "We will focus on the analysis of parallelism and distribution costs of algorithms.\n",
    "\n",
    "__Important Note__\n",
    "\n",
    "You are expected to use Python and Spark for this assignment. You cannot use any libraries beyond those already provided in Python. You can use only the built-in constructs of PySpark and are not allowed to use mllib or any other Spark library.\n",
    "\n",
    "__Brief Introduction to Spark__\n",
    "\n",
    "Spark is a data science software that allows you to write your data processing code in Scala, Python, or Java. The data is loaded as a Resilient Distributed Database (RDD) from either the local filesystem or HDFS. RDDs can be converted into other RDDs using transformations such as map, filter, reduceByKey, etc. The evaluation of RDDs is lazy i.e. the required result wonâ€™t be evaluated until you explicitly invoke an action indicating that you need the result. This allows Spark to optimize the execution of transformations scheduled on RDDs.\n",
    "Another useful feature of Spark is in-memory processing. You can specify that you want to cache an RDD in memory if you intend to reuse the RDD through multiple iterations of your data processing job. The full set of transformations that convert one RDD into another, and actions which force the calculation of a result can be found in the Spark programming guide. The programming guide is also a good introduction to Spark. A more detailed RDD API reference with examples can be found here. If you prefer a lecture, you can try the tutorial from Spark Summit 2016 available here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part I** Preliminaries\n",
    "\n",
    "- Setup and Quick Start\n",
    "- Learn pyspark (RDD, Dataframe, ...)\n",
    "- Prepare Dataset (MovieLens and Wikipedia)\n",
    "- Data Mining\n",
    "\n",
    "**Part II** Recommender System - Big Matrix Factorization using Spark\n",
    "\n",
    "- What is a recommender system?\n",
    "- Non-negative matrix factorization (NMF)\n",
    "- Stochastic Gradient Descent (SGD) for regularized NMF wtih $L_2$ penalty\n",
    "    - prgs skeleton\n",
    "    - stopping\n",
    "    - algorithm analysis (loss vs. time, prediction error vs. time, $\\lambda$ vs. final rmsi)\n",
    "\n",
    "**Part III** Text Categorization\n",
    "\n",
    "- What is text categorization?\n",
    "- Logistic regression\n",
    "- Proximal algorithm for regularized logistic regression wtih $L_2$ and $L_1$ penalties\n",
    "    - skeleton\n",
    "    - stropping\n",
    "    - algorithms analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The three hands-on exercices are all presented via Jupyter notebooks. Below are instructions on how to set up the environment using [Docker](http://www.docker.com). \n",
    "\n",
    "* Install Docker on your machine.\n",
    "\n",
    "  You should check your installation by running (open a terminal):  \n",
    "  \n",
    "  ```\n",
    "  docker run hello-world\n",
    "  ```\n",
    "\n",
    "* To install Jupyter Notebook with Spark 2.0, start the Docker image by running this:\n",
    "\n",
    "  ```\n",
    "  docker run -p 8888:8888 -p 4040:4040 -v $(pwd)/notebook:/home/jovyan/work  jupyter/pyspark-notebook:latest\n",
    "  ```\n",
    "  \n",
    "  ```\n",
    "  docker run -p 8888:8888 -p 4040:4040 -v $(pwd)/notebook:/home/jovyan/work -e GRANT_SUDO=yes --user root ezamir/jupyter-spark-2.0:latest\n",
    "  ```\n",
    "  \n",
    "    - Notice on Docker options\n",
    "\n",
    "    `-p 8888:8888` opens the port for the Jupyter Notebook UI.\n",
    "\n",
    "    `-p 4040:4040` opens the port for the Spark Monitoring and Instrumentation UI.\n",
    "\n",
    "    `-v /notebook:/home/jovyan/work` mounts the default working directory on the host to preserve your work even when the container is destroyed and recreated (e.g., during an upgrade).\n",
    "    \n",
    "\n",
    "* Open Jupyter - open browser to notebook link (http://localhost:8888/)\n",
    "\n",
    "* Upload the hands-on exercises notebooks - Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking your installation\n",
    "\n",
    "You can run the following code to check the versions of the packages on your system. In Jupyter notebook, press `shift` and `return` together to execute the contents of a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "numpy: 1.10.4\n",
      "scipy: 0.17.1\n",
      "matplotlib: 1.5.1\n",
      "scikit-learn: 0.17.1\n",
      "pandas: 0.19.0\n",
      "Ipython version:5.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys # determine Python version number\n",
    "print('Python version ' + sys.version)\n",
    "\n",
    "import numpy\n",
    "print('numpy:', numpy.__version__)\n",
    "\n",
    "import scipy\n",
    "print('scipy:', scipy.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print('matplotlib:', matplotlib.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "\n",
    "import pandas\n",
    "print('pandas:', pandas.__version__)\n",
    "\n",
    "import IPython\n",
    "print(\"Ipython version:\" + str(IPython.__version__))\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(\"local[*]\")\n",
    "print(\"pyspark version:\" + str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "- **[Jupyter Notebook](http://jupyter.org)** - Open source, interactive data science and scientific computing.\n",
    "- **[Apache Spark](http://spark.apache.org)** - Fast and general engine for large-scale data processing. \n",
    "- **[Docker](http://www.docker.com)** - Open-source project that automates the deployment of applications inside software containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
