{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part III - Recommender Systems (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based algorithms on the very well known matrix factorization problem which is the most prominent approach for build a _Recommender Systems_.\n",
    "\n",
    "Our goal is to implement Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "The problem of matrix factorization for collaborative filtering captured much attention, especially after the [Netflix prize](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf). The premise behind this approach is to approximate a large rating matrix $R$ with the multiplication of two low-dimensional factor matrices $P$ and $Q$, i.e. $R \\approx \\hat{R} = P^TQ$, that model respectively users and items in some latent space. For instance, matrix $R$ has dimension $m \\times  n$ where $m$ and $n$ are restrictively the number of users and items, both large; while $P$ has size $m \\times  k$ and contains user information in a latent space of size $k<<m,n$, $Q$ has size $n\\times k$ and contains item information in the same latent space of size $k << m,n$. Typical values for $m, n$ are $10^6$ while $k$ is in the tens.\n",
    "\n",
    "For a pair of user and item $(u_i,i_j)$ for which a rating $r_{ij}$ exists, a common approach approach is based on the minimization of the $\\ell_2$-regularized quadratic error:\n",
    "$$  \\ell_{u_i,i_j}(P,Q)= \\left(r_{ij} - p_{i}^{\\top}q_{j}\\right)^2 + \\lambda(|| p_{i} ||^{2} + || q_{j} ||^2 )  $$\n",
    "where $p_i$ is the column vector composed of the $i$-th line of $P$ and  $\\lambda\\geq 0$ is a regularization parameter. The whole matrix factorization problem thus writes\n",
    "$$ \\min_{P,Q} \\sum_{i,j : r_{ij} \\text{exists}}  \\ell_{u_i,i_j}(P,Q). $$\n",
    "Note that the error $ \\ell_{u_i,i_j}(P,Q)$ depends only on $P$ and $Q$ through $p_{i}$ and $q_{j}$; however, item $i_j$ may also be rated by user $u_{i'}$ so that the optimal factor $q_{j}$ depends on both $p_{i}$ and $p_{i'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=MSIAM part III - Matrix Factorization, master=local[*]) created by __init__ at <ipython-input-1-8ff3678c3768>:8 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8ff3678c3768>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSIAM part III - Matrix Factorization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 259\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=MSIAM part III - Matrix Factorization, master=local[*]) created by __init__ at <ipython-input-1-8ff3678c3768>:8 "
     ]
    }
   ],
   "source": [
    "# set up spark environment (Using Spark Local Mode)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part III - Matrix Factorization\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface by simply opening http://localhost:4040 in a web browser.\n",
    "\n",
    "We will capitalize on the first lab and take the MovieLens dataset, and thus the RDD routines we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1000209 ratings from 6040 users on 3706 movies.\n",
      "\n",
      "We have 6040 users, 3952 movies and the rating matrix has 4.190221 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parseRating(line):\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "\n",
    "def parseMovie(line):\n",
    "    fields = line.split(\"::\")\n",
    "    return int(fields[0]), fields[1], fields[2]\n",
    "\n",
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"data/movielens/medium/\"\n",
    "\n",
    "\n",
    "# movies is an RDD of (movieID, title, genres)\n",
    "moviesRDD = sc.textFile(movieLensHomeDir + \"movies.dat\").map(parseMovie).setName(\"movies\").cache()\n",
    "\n",
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir + \"ratings.dat\").map(parseRating).setName(\"ratings\").cache()\n",
    "\n",
    "numRatings = ratingsRDD.count()\n",
    "numUsers = ratingsRDD.map(lambda r: r[0]).distinct().count()\n",
    "numMovies = ratingsRDD.map(lambda r: r[1]).distinct().count()\n",
    "print(\"We have %d ratings from %d users on %d movies.\\n\" % (numRatings, numUsers, numMovies))\n",
    "\n",
    "M = ratingsRDD.map(lambda r: r[0]).max()\n",
    "N = ratingsRDD.map(lambda r: r[1]).max()\n",
    "matrixSparsity = float(numRatings)/float(M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % (M, N, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Descent Algorithms\n",
    "\n",
    "The goal here is to \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement gradient algorithms.\n",
    "3. Observe the prediction accuracy of the developed methods.\n",
    "\n",
    "__Question 1__\n",
    "\n",
    "> Split (ramdomly) the dataset into training versus testing sample. We learn over 70% (for example) of the users, we test over the rest.\n",
    "\n",
    "> Define a routine that returns the predicted rating from factor matrices. Form a RDD with the following elements `(i,j,true rating,predicted rating)`. \n",
    "\n",
    "> Define a routine that returns the Mean Square Error (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the dataset :  1000209\n",
      "The number of examples in the training dataset :  699709\n",
      "The number of examples in the testing dataset :  300500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "trainingSample, testingSample = ratingsRDD.randomSplit([70, 30])\n",
    "print(\"The number of examples in the dataset : \",ratingsRDD.count())\n",
    "print(\"The number of examples in the training dataset : \",trainingSample.count())\n",
    "print(\"The number of examples in the testing dataset : \",testingSample.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres of movie \"Pocahontas (1995)\":\n",
      " ['Animation', \"Children's\", 'Musical', 'Romance'] \n",
      "\n",
      "Five movies from genre \"Comedy\":\n",
      " ['Toy Story (1995)' 'Grumpier Old Men (1995)' 'Waiting to Exhale (1995)'\n",
      " 'Father of the Bride Part II (1995)' 'Sabrina (1995)'] \n",
      "\n",
      "There are 18 different genres:\n",
      " ['Musical', 'Fantasy', 'Documentary', \"Children's\", 'Horror', 'Drama', 'Thriller', 'Action', 'Sci-Fi', 'Western', 'Romance', 'Adventure', 'Film-Noir', 'Crime', 'War', 'Animation', 'Mystery', 'Comedy'] \n",
      "\n",
      "Check from dictionnary:\n",
      " 18 Genres:\n",
      " dict_keys([\"Children's\", 'Romance', 'Fantasy', 'Comedy', 'Thriller', 'Western', 'Action', 'War', 'Sci-Fi', 'Drama', 'Musical', 'Film-Noir', 'Horror', 'Documentary', 'Animation', 'Mystery', 'Adventure', 'Crime']) \n",
      "\n",
      "Number of movies having \"Comedy\" in their genres : 1200\n",
      "Check from dictionnary: 1200\n",
      "Number of movies having Musical in their genres : 114\n",
      "Check from RDD, the number of movies having Musical in their genres: 114\n"
     ]
    }
   ],
   "source": [
    "# Create moviesGenresRDD, a new RDD of (genre, list_of_movies)\n",
    "genres = moviesRDD.map(lambda x : x[2]).flatMap(lambda x : x.split(\"|\")).distinct()\n",
    "\n",
    "def getGenresOfMovie(movie):\n",
    "    return moviesRDD.filter(lambda x : x[1] == movie).map(lambda y : y[2]).flatMap(lambda x : x.split(\"|\")).collect()\n",
    "    \n",
    "def getMoviesFromGenre(genre):\n",
    "    return moviesRDD.filter(lambda x : genre in x[2]).map(lambda y : y[1]).collect()\n",
    "\n",
    "genres_dict = {}\n",
    "for g in genres.collect() :\n",
    "    genres_dict_temp = {g:getMoviesFromGenre(g)}\n",
    "    genres_dict.update(genres_dict_temp)\n",
    "\n",
    "moviesGenresRDD = genres.map(lambda genre : (genre,genres_dict[genre]))\n",
    "\n",
    "##TEST\n",
    "g = getGenresOfMovie(\"Pocahontas (1995)\")\n",
    "print(\"Genres of movie \\\"Pocahontas (1995)\\\":\\n\",g,\"\\n\")\n",
    "m = getMoviesFromGenre(\"Comedy\")\n",
    "print(\"Five movies from genre \\\"Comedy\\\":\\n\",np.asarray(m)[:5],\"\\n\")\n",
    "print(\"There are\",genres.count(),\"different genres:\\n\",genres.collect(),\"\\n\")\n",
    "print(\"Check from dictionnary:\\n\",len(genres_dict.keys()),\"Genres:\\n\",genres_dict.keys(),\"\\n\")\n",
    "print(\"Number of movies having \\\"Comedy\\\" in their genres :\",moviesRDD.filter(lambda x : \"Comedy\" in x[2]).count())\n",
    "print(\"Check from dictionnary:\",len(genres_dict[\"Comedy\"]))\n",
    "print(\"Number of movies having\",moviesGenresRDD.first()[0],\"in their genres :\",moviesRDD.filter(lambda x : \"Musical\" in x[2]).count())\n",
    "print(\"Check from RDD, the number of movies having\",moviesGenresRDD.first()[0],\"in their genres:\",len(moviesGenresRDD.first()[1]))\n",
    "\n",
    "#genres_dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define factors = Genres (k = 18 factors)\n",
    "\n",
    "# How to attribute values to factors\n",
    "# For vector q_i:\n",
    "#    each movie (item), have factor_value = 1 for its genres and -1 for others \n",
    "# For vector p_u:\n",
    "#    each user, have factor_value = Average_ratings_of_movies_of_factor = ratings / number_of_movies_of_factor\n",
    "  \n",
    "movies_dict = {}\n",
    "for i,title in moviesRDD.map(lambda x : (x[0],x[1])).collect():\n",
    "    movies_dict_temp = {i:title}\n",
    "    movies_dict.update(movies_dict_temp)\n",
    "\n",
    "def update_factors(movieIdListOfGenres,rating,p_u):\n",
    "    for f in movieIdListOfGenres:\n",
    "        p_u[f] = (p_u[f][0]+1, p_u[f][1] + rating)\n",
    "    return p_u\n",
    "\n",
    "def compute_final_p_u(p_u):\n",
    "    for f in genres.collect():\n",
    "        if p_u[f][0] != 0:\n",
    "            p_u[f] = (p_u[f][0]+1, p_u[f][1] / p_u[f][0])\n",
    "    return p_u\n",
    "\n",
    "def get_preferences(user,p_u):\n",
    "    pref = []\n",
    "    for g in genres.collect():\n",
    "        if p_u[g][0]>0:\n",
    "            pref.append(g)\n",
    "    return pref\n",
    "            \n",
    "\n",
    "def get_genres_list_of_movieId(movie):\n",
    "    movieTitle = movies_dict[movie]\n",
    "    #print(\"movie (id =\",movie,\"):\",movieTitle)\n",
    "    genreslist = moviesRDD.map(lambda x : x[2]).flatMap(lambda x : x.split(\"|\")).distinct()\n",
    "    return getGenresOfMovie(movieTitle)\n",
    "    \n",
    "def computePu(user):\n",
    "    p_u = {}\n",
    "    for g in genres.collect() :\n",
    "        p_u_temp = {g:(0,0)} # {genre, (number of movie with that genre rated; sum of eatings; average)}\n",
    "        p_u.update(p_u_temp)\n",
    "    userRatingsRDD = trainingSample.filter(lambda x : x[0] == user)\n",
    "    userMoviesList = userRatingsRDD.map(lambda x : x[1]).collect()\n",
    "    userRatingsList = userRatingsRDD.map(lambda x : x[2]).collect()\n",
    "    print(\"Number of movies rated:\",len(userMoviesList))\n",
    "    print(\"Movies rated:\",userMoviesList)\n",
    "    for l in range(0,len(userMoviesList)):\n",
    "        movieIdListOfGenres = get_genres_list_of_movieId(userMoviesList[l])\n",
    "        rating = userRatingsList[l]\n",
    "        #print(movieIdListOfGenres,\" ///// \", rating)\n",
    "        p_u = update_factors(movieIdListOfGenres,rating,p_u)\n",
    "    p_u = compute_final_p_u(p_u)\n",
    "    print(\"User preferences :\\n\",get_preferences(user,p_u))\n",
    "    return p_u\n",
    "    \n",
    "usersList = trainingSample.map(lambda r: r[0]).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeMovieRatingsAverage(movieId):\n",
    "    movieIdRatings = trainingSample.filter(lambda x : x[1] == movieId).map(lambda x : x[2])\n",
    "    movieIdRatingsAverage = movieIdRatings.mean()\n",
    "    print(\"The average of ratings (for the movieId =\",movieId,\")\",movieIdRatingsAverage)\n",
    "    return movieIdRatingsAverage\n",
    "\n",
    "def computeQi(item):\n",
    "    q_i = {}\n",
    "    for g in genres.collect() :\n",
    "        q_i_temp = {g:0} \n",
    "        q_i.update(q_i_temp)\n",
    "        \n",
    "    itemGenres = get_genres_list_of_movieId(item)\n",
    "    for genre in itemGenres:\n",
    "        q_i[genre] = computeMovieRatingsAverage(item)/len(itemGenres)\n",
    "    return q_i       \n",
    "    \n",
    "itemsList = moviesRDD.map(lambda r: r[0]).distinct().collect()\n",
    "print(itemsList)\n",
    "#for i in itemsList:\n",
    "#    print(getGenresOfMovie(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All genres:\n",
      " ['Musical', 'Fantasy', 'Documentary', \"Children's\", 'Horror', 'Drama', 'Thriller', 'Action', 'Sci-Fi', 'Western', 'Romance', 'Adventure', 'Film-Noir', 'Crime', 'War', 'Animation', 'Mystery', 'Comedy'] \n",
      "\n",
      "User = 1\n",
      "Movie = Christmas Story, A (1983)\n",
      "Number of movies rated: 42\n",
      "Movies rated: [661, 914, 3408, 2355, 1287, 2804, 594, 919, 2918, 1035, 2791, 2018, 3105, 2797, 2321, 720, 527, 2340, 48, 1097, 1721, 1545, 745, 2294, 3186, 588, 1907, 1836, 1022, 2762, 150, 1, 1961, 1962, 2692, 260, 1029, 1207, 531, 3114, 608, 1246]\n",
      "User preferences :\n",
      " ['Musical', 'Fantasy', \"Children's\", 'Drama', 'Thriller', 'Action', 'Sci-Fi', 'Romance', 'Adventure', 'Crime', 'War', 'Animation', 'Comedy']\n",
      "The average of ratings (for the movieId = 2804 ) 4.216931216931217\n",
      "The average of ratings (for the movieId = 2804 ) 4.216931216931217\n",
      "\n",
      "Pu = \n",
      " [ 4.22222222  4.          0.          4.26666667  0.          4.38888889\n",
      "  3.66666667  4.33333333  4.          0.          3.8         4.33333333\n",
      "  0.          4.          5.          4.14285714  0.          4.1       ]\n",
      "\n",
      "Qi =\n",
      " [ 0.          0.          0.          0.          0.          2.10846561\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          2.10846561]\n",
      "\n",
      "Dot product = 17.8985302763\n",
      "True rating = [(1, 2804, 5.0)]\n"
     ]
    }
   ],
   "source": [
    "def dict2list(dic,col):\n",
    "    index = 0\n",
    "    _list = np.zeros(18)\n",
    "    for f in genres.collect():\n",
    "        if col != 0:\n",
    "            _list[index] = dic[f][col]\n",
    "        else:\n",
    "            _list[index] = dic[f]\n",
    "        index = index + 1\n",
    "    return _list\n",
    "    \n",
    "print(\"All genres:\\n\",genres.collect(),\"\\n\")\n",
    "\n",
    "user = 1\n",
    "item = 2804\n",
    "\n",
    "print(\"User =\",user) \n",
    "print(\"Movie =\",movies_dict[item])\n",
    "\n",
    "Pu = dict2list(computePu(user),1)\n",
    "Qi = dict2list(computeQi(item),0)\n",
    "\n",
    "print(\"\\nPu = \\n\",Pu) \n",
    "print(\"\\nQi =\\n\",Qi)\n",
    "\n",
    "ps = np.vdot(Pu,Qi)\n",
    "print(\"\\nDot product =\",ps)\n",
    "\n",
    "realRating = ratingsRDD.filter(lambda x : x[0]==user).filter(lambda x : x[1]==item).collect()\n",
    "print(\"True rating =\",realRating)\n",
    "# western movie eg : \"Wild Bill (1995)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__\n",
    "\n",
    "> Derive the update rules for gradient descent. \n",
    "\n",
    "> Implement a (full) gradient algorithm in `Python` on the training set.  Take a step size (learning rate) $\\gamma=0.001$ and stop after a specified number of iterations. Investigate the latent space size (e.g. $K=2,5,10,50$).\n",
    "\n",
    "> Provide plots and explanations for your experiments. \n",
    "\n",
    "> Try to parrallelize it so that the code can be run using `PySpark`. What do you conclude?\n",
    "\n",
    "Stochastic Gradient Descent (SGD) simply does away with the expectation in the update and computes the gradient of the parameters using only a single or a few training examples. In SGD the learning rate $\\gamma$ is typically much smaller than a corresponding learning rate in batch gradient descent because there is much more variance in the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "> Implement stochastic gradient descent algorithm for Matrix Factorization.\n",
    "\n",
    "> Provide plots and explanations for your experiments.\n",
    "\n",
    "> Compare and discuss the results with the (full) gradient algorithm in terms of MSE versus full data passes.\n",
    "\n",
    "> Discuss the stepsize choice of SGD (e.g. constant v.s. 1/`nb_iter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent (DSGD) in Spark. \n",
    "The algorithm is described in the following article: <br \\><br \\>\n",
    "_Gemulla, R., Nijkamp, E., Haas, P. J., & Sismanis, Y. (2011). Large-scale matrix factorization with distributed stochastic gradient descent. New York, USA._<br \\><br \\>\n",
    "The paper sets forth a solution for matrix factorization using minimization of sum of local losses.  The solution involves dividing the matrix into strata for each iteration and performing sequential stochastic gradient descent within each stratum in parallel.  DSGD is a fully distributed algorithm, i.e. both the data matrix $R$ and factor matrices $P$ and $Q$ can be carefully split and distributed to multiple workers for parallel computation without communication costs between the workers. Hence, it is a good match for implementation in a distributed in-memory data processing system like Spark. \n",
    "\n",
    "__Question 4__\n",
    "\n",
    "> Implement a `PySpark` version of DSGD.\n",
    "\n",
    "> Test on different number of cores on a local machine (1 core, 2 cores, 4 cores). Ran the ALS method already implemented in MLlib as a reference for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
