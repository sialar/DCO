{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was create by Franck Iutzeler, Jerome Malick and Yann Vernaz (2016).</i></small>\n",
    "<!-- Credit (images) Jeffrey Keating Thompson. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)</h3></center>\n",
    "<hr>\n",
    "<center><h1>Convex and distributed optimization</h1></center>\n",
    "<center><h2>Part III - Recommender Systems (3h + 3h home work)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In this Lab, we will investigate some gradient-based algorithms on the very well known matrix factorization problem which is the most prominent approach for build a _Recommender Systems_.\n",
    "\n",
    "Our goal is to implement Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "The problem of matrix factorization for collaborative filtering captured much attention, especially after the [Netflix prize](https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf). The premise behind this approach is to approximate a large rating matrix $R$ with the multiplication of two low-dimensional factor matrices $P$ and $Q$, i.e. $R \\approx \\hat{R} = P^TQ$, that model respectively users and items in some latent space. For instance, matrix $R$ has dimension $m \\times  n$ where $m$ and $n$ are restrictively the number of users and items, both large; while $P$ has size $m \\times  k$ and contains user information in a latent space of size $k<<m,n$, $Q$ has size $n\\times k$ and contains item information in the same latent space of size $k << m,n$. Typical values for $m, n$ are $10^6$ while $k$ is in the tens.\n",
    "\n",
    "For a pair of user and item $(u_i,i_j)$ for which a rating $r_{ij}$ exists, a common approach approach is based on the minimization of the $\\ell_2$-regularized quadratic error:\n",
    "$$  \\ell_{u_i,i_j}(P,Q)= \\left(r_{ij} - p_{i}^{\\top}q_{j}\\right)^2 + \\lambda(|| p_{i} ||^{2} + || q_{j} ||^2 )  $$\n",
    "where $p_i$ is the column vector composed of the $i$-th line of $P$ and  $\\lambda\\geq 0$ is a regularization parameter. The whole matrix factorization problem thus writes\n",
    "$$ \\min_{P,Q} \\sum_{i,j : r_{ij} \\text{exists}}  \\ell_{u_i,i_j}(P,Q). $$\n",
    "Note that the error $ \\ell_{u_i,i_j}(P,Q)$ depends only on $P$ and $Q$ through $p_{i}$ and $q_{j}$; however, item $i_j$ may also be rated by user $u_{i'}$ so that the optimal factor $q_{j}$ depends on both $p_{i}$ and $p_{i'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up spark environment (Using Spark Local Mode)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MSIAM part III - Matrix Factorization\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind you that you can access this interface by simply opening http://localhost:4040 in a web browser.\n",
    "\n",
    "We will capitalize on the first lab and take the MovieLens dataset, and thus the RDD routines we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1000209 ratings from 6040 users on 3706 movies.\n",
      "\n",
      "We have 6040 users, 3952 movies and the rating matrix has 4.190221 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parseRating(line):\n",
    "    fields = line.split('::')\n",
    "    return int(fields[0]), int(fields[1]), float(fields[2])\n",
    "\n",
    "def parseMovie(line):\n",
    "    fields = line.split(\"::\")\n",
    "    return int(fields[0]), fields[1], fields[2]\n",
    "\n",
    "# path to MovieLens dataset\n",
    "movieLensHomeDir=\"data/movielens/medium/\"\n",
    "\n",
    "# ratings is an RDD of (userID, movieID, rating)\n",
    "ratingsRDD = sc.textFile(movieLensHomeDir + \"ratings.dat\").map(parseRating).setName(\"ratings\").cache()\n",
    "\n",
    "numRatings = ratingsRDD.count()\n",
    "numUsers = ratingsRDD.map(lambda r: r[0]).distinct().count()\n",
    "numMovies = ratingsRDD.map(lambda r: r[1]).distinct().count()\n",
    "print(\"We have %d ratings from %d users on %d movies.\\n\" % (numRatings, numUsers, numMovies))\n",
    "\n",
    "M = ratingsRDD.map(lambda r: r[0]).max()\n",
    "N = ratingsRDD.map(lambda r: r[1]).max()\n",
    "matrixSparsity = float(numRatings)/float(M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % (M, N, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Descent Algorithms\n",
    "\n",
    "The goal here is to \n",
    "1. Compute gradients of the loss functions.\n",
    "2. Implement gradient algorithms.\n",
    "3. Observe the prediction accuracy of the developed methods.\n",
    "\n",
    "__Question 1__\n",
    "\n",
    "> Split (ramdomly) the dataset into training versus testing sample. We learn over 70% (for example) of the users, we test over the rest.\n",
    "\n",
    "> Define a routine that returns the predicted rating from factor matrices. Form a RDD with the following elements `(i,j,true rating,predicted rating)`. \n",
    "\n",
    "> Define a routine that returns the Mean Square Error (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples in the dataset :  1000209\n",
      "The number of examples in the training dataset :  1000209\n",
      "The number of examples in the testing dataset :  1000209\n",
      "P contains user information in a latent spaceof size 3.\n",
      " [[  1.00000000e+00   1.19300000e+03   5.00000000e+00]\n",
      " [  1.00000000e+00   6.61000000e+02   3.00000000e+00]\n",
      " [  1.00000000e+00   9.14000000e+02   3.00000000e+00]\n",
      " ..., \n",
      " [  6.04000000e+03   5.62000000e+02   5.00000000e+00]\n",
      " [  6.04000000e+03   1.09600000e+03   4.00000000e+00]\n",
      " [  6.04000000e+03   1.09700000e+03   4.00000000e+00]]\n",
      "Q contains item information in the same latent space of size 3. [['1' 'Toy Story (1995)' \"Animation|Children's|Comedy\"]\n",
      " ['2' 'Jumanji (1995)' \"Adventure|Children's|Fantasy\"]\n",
      " ['3' 'Grumpier Old Men (1995)' 'Comedy|Romance']\n",
      " ['4' 'Waiting to Exhale (1995)' 'Comedy|Drama']\n",
      " ['5' 'Father of the Bride Part II (1995)' 'Comedy']]\n",
      "3883\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "trainingSample, testingSample = ratingsRDD.randomSplit([70, 30])\n",
    "print(\"The number of examples in the dataset : \",ratingsRDD.count())\n",
    "print(\"The number of examples in the training dataset : \",ratingsRDD.count())\n",
    "print(\"The number of examples in the testing dataset : \",ratingsRDD.count())\n",
    "\n",
    "moviesRDD = sc.textFile(movieLensHomeDir + \"movies.dat\").map(parseMovie).setName(\"movies\").cache()\n",
    "\n",
    "P = np.asmatrix(ratingsRDD.collect())\n",
    "print(\"P contains user information in a latent spaceof size 3.\\n\",P)\n",
    "Q = np.asmatrix(moviesRDD.take(5))\n",
    "print(\"Q contains item information in the same latent space of size 3.\",Q)\n",
    "\n",
    "moviesDistinct = moviesRDD.distinct().count()\n",
    "print(moviesDistinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__\n",
    "\n",
    "> Derive the update rules for gradient descent. \n",
    "\n",
    "> Implement a (full) gradient algorithm in `Python` on the training set.  Take a step size (learning rate) $\\gamma=0.001$ and stop after a specified number of iterations. Investigate the latent space size (e.g. $K=2,5,10,50$).\n",
    "\n",
    "> Provide plots and explanations for your experiments. \n",
    "\n",
    "> Try to parrallelize it so that the code can be run using `PySpark`. What do you conclude?\n",
    "\n",
    "Stochastic Gradient Descent (SGD) simply does away with the expectation in the update and computes the gradient of the parameters using only a single or a few training examples. In SGD the learning rate $\\gamma$ is typically much smaller than a corresponding learning rate in batch gradient descent because there is much more variance in the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 3__\n",
    "> Implement stochastic gradient descent algorithm for Matrix Factorization.\n",
    "\n",
    "> Provide plots and explanations for your experiments.\n",
    "\n",
    "> Compare and discuss the results with the (full) gradient algorithm in terms of MSE versus full data passes.\n",
    "\n",
    "> Discuss the stepsize choice of SGD (e.g. constant v.s. 1/`nb_iter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement Large-Scale Matrix Factorization with Distributed Stochastic Gradient Descent (DSGD) in Spark. \n",
    "The algorithm is described in the following article: <br \\><br \\>\n",
    "_Gemulla, R., Nijkamp, E., Haas, P. J., & Sismanis, Y. (2011). Large-scale matrix factorization with distributed stochastic gradient descent. New York, USA._<br \\><br \\>\n",
    "The paper sets forth a solution for matrix factorization using minimization of sum of local losses.  The solution involves dividing the matrix into strata for each iteration and performing sequential stochastic gradient descent within each stratum in parallel.  DSGD is a fully distributed algorithm, i.e. both the data matrix $R$ and factor matrices $P$ and $Q$ can be carefully split and distributed to multiple workers for parallel computation without communication costs between the workers. Hence, it is a good match for implementation in a distributed in-memory data processing system like Spark. \n",
    "\n",
    "__Question 4__\n",
    "\n",
    "> Implement a `PySpark` version of DSGD.\n",
    "\n",
    "> Test on different number of cores on a local machine (1 core, 2 cores, 4 cores). Ran the ALS method already implemented in MLlib as a reference for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
